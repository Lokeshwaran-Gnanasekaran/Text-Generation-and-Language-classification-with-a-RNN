{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash ./download_language.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n",
      "train len:  1003854\n",
      "test len:  111540\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = 'language_data/shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " way, when\n",
      "for a day of kings' entreaties a mother should not\n",
      "sell him an hour from her beholding, I, considering\n",
      "how honour would become such a person. that it was\n",
      "no better than picture-like to hang \n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 300\n",
    "n_layers = 2\n",
    "learning_rate = 0.0001\n",
    "model_type = 'lstm'\n",
    "print_every = 50\n",
    "plot_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    batch_size = input.size(0)\n",
    "    device = input.device\n",
    "    hidden = rnn.init_hidden(batch_size, device)\n",
    "\n",
    "    # Zero the gradients for the optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Initialize the loss\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Loop over each time step in the input sequence\n",
    "    for t in range(input.size(1)):\n",
    "        # Select the character at time step t\n",
    "        input_t = input[:, t]  # Shape: (batch_size)\n",
    "        target_t = target[:, t]  # Shape: (batch_size)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output, hidden = rnn(input_t, hidden)  # output shape: (batch_size, output_size)\n",
    "\n",
    "        # Compute loss for the time step\n",
    "        loss_t = criterion(output, target_t)\n",
    "        total_loss += loss_t\n",
    "\n",
    "    # Average the loss over all time steps\n",
    "    average_loss = total_loss / input.size(1)\n",
    "\n",
    "    # Backward pass and optimizer step\n",
    "    average_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    ##########       END      ##########\n",
    "\n",
    "    return average_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n",
      "[0m 17s (50 1%) train loss: 3.5650, test_loss: 3.5844]\n",
      "WheEWy$=ns,n ^eth esre^!n oiseorelnt eh ewrsen iilr  N n  ea stha)\n",
      "h0 aD_.\n",
      "ehaixewp\n",
      " iwags entrsm ntz  \n",
      "\n",
      "[0m 34s (100 2%) train loss: 3.3697, test_loss: 3.4082]\n",
      "Whft*BZ seugtifeooz  o wa ataatihfiic  ueaw n ss.yu oeFlaioet h e\n",
      "esg eai  uasano e  elrbe  olre ad\n",
      "tt \n",
      "\n",
      "[0m 52s (150 3%) train loss: 3.3226, test_loss: 3.3455]\n",
      "Wh)wi9irltans eatumTt  tBenfai m h hus:rs urdo:   hoeaothr, oei dm we g  imieutansGd ub eo o  lc lis A \n",
      "\n",
      "[1m 9s (200 4%) train loss: 3.2354, test_loss: 3.2617]\n",
      "Whk:rom  ewneohhhhh e,n   e  heaehahmera\n",
      "\n",
      " N ons\n",
      "hheet et ie ae\n",
      " re l eayieaor  hei speh p so\n",
      "ar of;oe \n",
      "\n",
      "[1m 26s (250 5%) train loss: 3.0913, test_loss: 3.1029]\n",
      "Wh<miwhk  oahy ;a iut sarois arhsn nooeg:dMAeSC'e\n",
      "e la ug iithbn,e srt fao\n",
      "hr ttue.\n",
      "\n",
      "wC\n",
      "W\n",
      "u:n\n",
      "lehda Ne \n",
      "\n",
      "[1m 44s (300 6%) train loss: 2.9776, test_loss: 2.9534]\n",
      "Whgile \n",
      "a\n",
      "\n",
      "r;B, woteddne arer oethl ehl hen tshc, hh lae cfcmrks kdrcI hs\n",
      "\n",
      "nnmA atinl r ttho tiinutno  \n",
      "\n",
      "[2m 1s (350 7%) train loss: 2.8282, test_loss: 2.8138]\n",
      "Wh+\n",
      "hchd tose ncmikl aie se tif oelted tais the terrs\n",
      "oa roet\n",
      ",,\n",
      "ga au aioe Te ie poes Bo whey\n",
      "\n",
      "\n",
      "ihr n \n",
      "\n",
      "[2m 19s (400 8%) train loss: 2.7196, test_loss: 2.7071]\n",
      "Wh#mgPrree eao fts lou; hereg, aout ftot beoce ceou, mho th.\n",
      "\n",
      "Tife se toaue tae wead the tht ah iy rhe \n",
      "\n",
      "[2m 36s (450 9%) train loss: 2.5844, test_loss: 2.6176]\n",
      "Whk!\n",
      "-hWwe heut ir oors-thi ior\n",
      ", Munlles\n",
      ", ghdPmranlw loy tos mhels hord meedsWnf;\n",
      "\n",
      "Ancor, tusr har u \n",
      "\n",
      "[2m 53s (500 10%) train loss: 2.5470, test_loss: 2.5434]\n",
      "WhTeif: hon he yo th! som at heum whe hor\n",
      "A.\n",
      "\n",
      "YU:UY\n",
      "LWI yehet acis the; so pusnclat nutins ange.\n",
      "\n",
      "\n",
      "caA \n",
      "\n",
      "[3m 11s (550 11%) train loss: 2.4757, test_loss: 2.4649]\n",
      "Whies tho ifrnthit oot s, wach hade she;\n",
      "heat hor hod be khece srouar meeses\n",
      "Lout il siss huwes he whe \n",
      "\n",
      "[3m 28s (600 12%) train loss: 2.4507, test_loss: 2.4287]\n",
      "Wheme basks tean,\n",
      "ond, the thes sliog in he tale bith ges\n",
      "Oani srot mead he the dakither sore kisl.\n",
      "Tp \n",
      "\n",
      "[3m 46s (650 13%) train loss: 2.3977, test_loss: 2.4034]\n",
      "Whin-Moshet fras mith riund,\n",
      "gou laus he al fonr bicum and arb'n\n",
      "\n",
      "oar se tlis wo gope bethe, I thand w \n",
      "\n",
      "[4m 3s (700 14%) train loss: 2.3684, test_loss: 2.3545]\n",
      "Whe theres mevhe dases:\n",
      "Ard wint bemed: coo gme lans on ciy invt ute\n",
      "Dor'n her.\n",
      "Bued, that are frals,  \n",
      "\n",
      "[4m 20s (750 15%) train loss: 2.3360, test_loss: 2.3133]\n",
      "Whau chirce hait to coudlcose bild and,\n",
      "Fy lith' neferith yo the thin ce manghy and, I Rounsind af mer \n",
      "\n",
      "[4m 38s (800 16%) train loss: 2.3173, test_loss: 2.3205]\n",
      "Whourt, at whe hill thous bot ii's;\n",
      "I pod the worke sitg bithe, siw dour, gaer wcousd ofo,\n",
      "\n",
      "Pothe soth \n",
      "\n",
      "[4m 55s (850 17%) train loss: 2.2865, test_loss: 2.3010]\n",
      "Who tind at hasg eod dot you Winesod the thpilges,\n",
      "And we sis ee dale share, soo, fifst ty owe fly wor \n",
      "\n",
      "[5m 12s (900 18%) train loss: 2.2768, test_loss: 2.2878]\n",
      "Wheen tore ou derereslaf rond poflan\n",
      "Sheut time, dowunat wand you mifh the, hils your dendeeinty\n",
      "The s \n",
      "\n",
      "[5m 30s (950 19%) train loss: 2.2568, test_loss: 2.2563]\n",
      "Whant; 'on tire sealcd gole'e poor,\n",
      "On ond, af the wives anr cithes be mere une;\n",
      "Wo Youl foy so feibl  \n",
      "\n",
      "[5m 48s (1000 20%) train loss: 2.2230, test_loss: 2.2352]\n",
      "Whes, theres want and be to coath?\n",
      "MFat perene cere? In, oul the anderor'd, Host tis thall I salt ind; \n",
      "\n",
      "[6m 6s (1050 21%) train loss: 2.1878, test_loss: 2.2379]\n",
      "Wher inear the weren\n",
      "Ciningulan.\n",
      "\n",
      "BHIRLAERE\n",
      "MOI:\n",
      "Whey theas you'd heired hanith thay your woth!\n",
      "Whey t \n",
      "\n",
      "[6m 23s (1100 22%) train loss: 2.1962, test_loss: 2.2056]\n",
      "Whe I forrlond deron on thor ak ange\n",
      "Collad thou shary the shith in lronwirs-.\n",
      "\n",
      "LANFSW:\n",
      "CI I SIchlaum  \n",
      "\n",
      "[6m 41s (1150 23%) train loss: 2.1654, test_loss: 2.1991]\n",
      "Whe panlingo falv the hay co sute?\n",
      "\n",
      "BECEOUI:\n",
      "Bye daad, at pallo, an much if shere?\n",
      "\n",
      "LRINY I ELINEO:\n",
      "Th \n",
      "\n",
      "[7m 0s (1200 24%) train loss: 2.1604, test_loss: 2.1771]\n",
      "Wher gord sone ar soumt af ir mathe,\n",
      "And sont lath srist le the fane to searsts.\n",
      "\n",
      "CISLORRO:\n",
      "Nhar theam \n",
      "\n",
      "[7m 17s (1250 25%) train loss: 2.1691, test_loss: 2.1628]\n",
      "Whe whend hom have her henetine the whing!\n",
      "\n",
      "IUGETAS:\n",
      "Seer'llt sele for me the bese mice that mer\n",
      "Tull  \n",
      "\n",
      "[7m 34s (1300 26%) train loss: 2.1177, test_loss: 2.1394]\n",
      "Whill sear this ancoressent tinoussadt!\n",
      "\n",
      "LUCELES:\n",
      "Ye, castsed bisters ou hor.\n",
      "\n",
      "TORTINANS:\n",
      "Rome, are to \n",
      "\n",
      "[7m 52s (1350 27%) train loss: 2.1034, test_loss: 2.1558]\n",
      "Wher led thou wen, hot here that hall thy pron, of ouldt;\n",
      "Got what ous thee ball's a dive frnon ame's, \n",
      "\n",
      "[8m 9s (1400 28%) train loss: 2.0839, test_loss: 2.1276]\n",
      "Whe doness, and ard komt as sheat Es\n",
      "Have shour thiss dyate dotherstange enpelce,\n",
      "Whiles not hprealc,  \n",
      "\n",
      "[8m 28s (1450 28%) train loss: 2.0802, test_loss: 2.1173]\n",
      "Whe perout handerourd hais weer the the bensed\n",
      "And solant and bort sker bece cond rose.\n",
      "\n",
      "PORGONS:\n",
      "Un t \n",
      "\n",
      "[8m 45s (1500 30%) train loss: 2.0657, test_loss: 2.0972]\n",
      "Whe datter and and thepe fore\n",
      "Wheme gher goond wirld at tham in the thale\n",
      "And padingt mrton this of th \n",
      "\n",
      "[9m 3s (1550 31%) train loss: 2.0557, test_loss: 2.0975]\n",
      "Whe gand han'ks stincune of you wher and;\n",
      "Whin puther sinker as the wast the wiste\n",
      "The wlist his whith \n",
      "\n",
      "[9m 20s (1600 32%) train loss: 2.0680, test_loss: 2.0894]\n",
      "When sseen tras with and stee enoled\n",
      "\n",
      "Mumith I verlory wour so the come shouln,\n",
      "When tisin he gound wh \n",
      "\n",
      "[9m 38s (1650 33%) train loss: 2.0519, test_loss: 2.0759]\n",
      "Whed do core wipe hoo hit troughy theus ant.\n",
      "\n",
      "LAMCAAS:\n",
      "Shan he the to his sfoth obre and sut chinger\n",
      "o \n",
      "\n",
      "[9m 55s (1700 34%) train loss: 2.0183, test_loss: 2.0759]\n",
      "Whe mave me lere carwing, and prioes louns,\n",
      "The for me that thase fele you goue to my thee rear,\n",
      "And w \n",
      "\n",
      "[10m 12s (1750 35%) train loss: 2.0269, test_loss: 2.0418]\n",
      "Whe wor I have somre prether loss sure,\n",
      "Thace, reven the foult the to king, me arse\n",
      "Clochald the thoug \n",
      "\n",
      "[10m 30s (1800 36%) train loss: 1.9741, test_loss: 2.0474]\n",
      "Whan andser's that thou someres lich the kere aured,\n",
      "To sat hit that afe youe foroufs of to hand,\n",
      "And  \n",
      "\n",
      "[10m 47s (1850 37%) train loss: 1.9837, test_loss: 2.0509]\n",
      "Whe wime thy dered, of martuing,\n",
      "Thay for\n",
      "wiwe not in miver south do his proreste gore\n",
      "Nerdow ard the  \n",
      "\n",
      "[11m 5s (1900 38%) train loss: 1.9738, test_loss: 2.0396]\n",
      "Whe antime.\n",
      "\n",
      "PUCINLHARG:\n",
      "Mererown ufath will oo hand bither stake the mave\n",
      "I preaping-t'y like astnouk \n",
      "\n",
      "[11m 23s (1950 39%) train loss: 1.9702, test_loss: 2.0013]\n",
      "Whear me anes low no that I lom Lice sulis ke hay;\n",
      "And lame toul sere the rotood? and shemes not\n",
      "Ald,  \n",
      "\n",
      "[11m 41s (2000 40%) train loss: 1.9411, test_loss: 2.0074]\n",
      "Whe ay hes poost in thene comen ore\n",
      "And, mine yout or the freid by fath in is paities\n",
      "And buchslant ml \n",
      "\n",
      "[11m 59s (2050 41%) train loss: 1.9547, test_loss: 1.9914]\n",
      "Whe pooth leachs.\n",
      "\n",
      "GLAORCES:\n",
      "Bod Weres'n, all hears word soor the sound then beverbe:\n",
      "I btay in blish  \n",
      "\n",
      "[12m 16s (2100 42%) train loss: 1.9278, test_loss: 2.0130]\n",
      "Whet hant be prime in wipttnen.\n",
      "I you will sow; thy merter the the came, a why dist freat,\n",
      "The preect  \n",
      "\n",
      "[12m 33s (2150 43%) train loss: 1.9074, test_loss: 1.9963]\n",
      "Whe dave in for preicgrde of the lave made,\n",
      "The prope tou is bore her drieng vave tine whe heath of\n",
      "Do \n",
      "\n",
      "[12m 51s (2200 44%) train loss: 1.8813, test_loss: 1.9837]\n",
      "Whers ave now my spenter.\n",
      "Hast in ghat be sore to and beater sice,\n",
      "To be waple sond this my pray my sa \n",
      "\n",
      "[13m 9s (2250 45%) train loss: 1.9133, test_loss: 1.9511]\n",
      "Whe conk, kall your who, eprod the hows,\n",
      "Bead strean his I will that thes ests forl,\n",
      "To the will cencu \n",
      "\n",
      "[13m 26s (2300 46%) train loss: 1.9054, test_loss: 1.9702]\n",
      "When hand, well keak for us hand.\n",
      "\n",
      "SLICIF ENARTE:\n",
      "In garnow of the mad in entin are wing deaks,\n",
      "That a \n",
      "\n",
      "[13m 44s (2350 47%) train loss: 1.8631, test_loss: 1.9775]\n",
      "Wheld this preame thears.\n",
      "\n",
      "MUTHERSAS:\n",
      "Hat it the poar-for,\n",
      "My hood in thou for the cind; surdy befifio \n",
      "\n",
      "[14m 1s (2400 48%) train loss: 1.9005, test_loss: 1.9386]\n",
      "Where, you couron his pyages,\n",
      "Thas the erved so hous the stend, will with bike\n",
      "Thitheid preacter bustr \n",
      "\n",
      "[14m 18s (2450 49%) train loss: 1.8572, test_loss: 1.9492]\n",
      "When be bussersed for what filld me,\n",
      "And knound poth of has catferteglay will hismen.\n",
      "\n",
      "SoRTICHERKE\n",
      "GRO \n",
      "\n",
      "[14m 36s (2500 50%) train loss: 1.8811, test_loss: 1.9345]\n",
      "Whers bote the ofter uppelter,\n",
      "And she will dome freat he word the not him.\n",
      "\n",
      "LORCISA:\n",
      "Wheen Lassiot as \n",
      "\n",
      "[14m 54s (2550 51%) train loss: 1.8287, test_loss: 1.9521]\n",
      "When batise, you in spull but wormung!\n",
      "-hou stay thacp then is for tige that thee;\n",
      "Be coof not not of  \n",
      "\n",
      "[15m 11s (2600 52%) train loss: 1.8498, test_loss: 1.9417]\n",
      "Whe, shalk with the vinganias not to me all,\n",
      "Set juth he gansel thy freath, of the treed he ape\n",
      "Mance  \n",
      "\n",
      "[15m 29s (2650 53%) train loss: 1.8349, test_loss: 1.9206]\n",
      "Whip with and thing patiss of garous will dlong all her doud,\n",
      "Ke pane tray hame some wiss liits nos ma \n",
      "\n",
      "[15m 46s (2700 54%) train loss: 1.8349, test_loss: 1.9308]\n",
      "Whith ben the latle proupe thee\n",
      "where, lest the pood canns of fray, ars listert hin\n",
      "Bust me must poy b \n",
      "\n",
      "[16m 4s (2750 55%) train loss: 1.8442, test_loss: 1.9173]\n",
      "Whing to reters, And thich thag the preent.\n",
      "As mon worlest mustard: in the pout his for housh,\n",
      "Whet ra \n",
      "\n",
      "[16m 21s (2800 56%) train loss: 1.8193, test_loss: 1.9022]\n",
      "Where that you lest's ciep, Coun yee,\n",
      "I man the word so nog she are as indenger\n",
      "Were thou sard of tor  \n",
      "\n",
      "[16m 38s (2850 56%) train loss: 1.8225, test_loss: 1.9003]\n",
      "Whild is a prister, bleint: ne mould him us ar lyow\n",
      "The good not me a dead, thou shalle is morest.\n",
      "Tha \n",
      "\n",
      "[16m 56s (2900 57%) train loss: 1.8084, test_loss: 1.9009]\n",
      "Where they seak, ow lidin with gones.\n",
      "Or us a prishadiousil, Worbull of Read.\n",
      "\n",
      "ORALEY BO:\n",
      "You ar achar \n",
      "\n",
      "[17m 13s (2950 59%) train loss: 1.8041, test_loss: 1.8987]\n",
      "Whis me\n",
      "porow a masing, wore of your are beand\n",
      "Be malle then, of the coonivery oferrow\n",
      "Mirire sage the \n",
      "\n",
      "[17m 31s (3000 60%) train loss: 1.8050, test_loss: 1.8834]\n",
      "Wher te ever good herfarcas the len\n",
      "Mave seal go the burdes cimls beet a this risce kord agpet.\n",
      "O' her \n",
      "\n",
      "[17m 49s (3050 61%) train loss: 1.7628, test_loss: 1.8896]\n",
      "Whe bust hear nimnerfoliof,\n",
      "Or one beting bither semon, their mace,\n",
      "O angerited the preagh I mar hear  \n",
      "\n",
      "[18m 6s (3100 62%) train loss: 1.7308, test_loss: 1.8687]\n",
      "Wher ip there;\n",
      "And this hack a nother my all to render.\n",
      "\n",
      "QUEEN EDIR\n",
      "SARIS:\n",
      "No lits of come poremone sa \n",
      "\n",
      "[18m 23s (3150 63%) train loss: 1.7554, test_loss: 1.8709]\n",
      "Whered frow that to fath his tourss,\n",
      "As I lave would gell id deeps stear\n",
      "And that I so bust of lay, I  \n",
      "\n",
      "[18m 41s (3200 64%) train loss: 1.7679, test_loss: 1.8691]\n",
      "Whath powas:\n",
      "That sheiled this is lave the wall of heartet\n",
      "Bomoming tho grad gonded mush hore's his hi \n",
      "\n",
      "[18m 58s (3250 65%) train loss: 1.7777, test_loss: 1.8726]\n",
      "Wheness beatiss for tten\n",
      "Yey he urong of the hand of uswing, at my bone\n",
      "I cerit, hear imselved and the \n",
      "\n",
      "[19m 17s (3300 66%) train loss: 1.7423, test_loss: 1.8548]\n",
      "Whis this worght be to the compons\n",
      "And a Lacker this lindue it can, at my low,\n",
      "And youther praice, sou \n",
      "\n",
      "[19m 34s (3350 67%) train loss: 1.7538, test_loss: 1.8556]\n",
      "Whack and with my lowt,\n",
      "Too a wift the Bigin ablorss of are to me\n",
      "pirnorle, thou thel, flet my leprema \n",
      "\n",
      "[19m 51s (3400 68%) train loss: 1.7555, test_loss: 1.8782]\n",
      "Whil not:\n",
      "Normand the efpraty shath he Morsuce on he calm,\n",
      "If in to the scemces, puck with I hang;\n",
      "Thi \n",
      "\n",
      "[20m 9s (3450 69%) train loss: 1.7267, test_loss: 1.8537]\n",
      "Whing sirdor all so should that thy sirt?\n",
      "\n",
      "LENTY:\n",
      "Themen fareman mand ble diveling fore all;\n",
      "And infor \n",
      "\n",
      "[20m 26s (3500 70%) train loss: 1.7079, test_loss: 1.8661]\n",
      "Whir's if it he sto now sinder, which hersick\n",
      "As in that bet shill loves the edeate thee, and under,\n",
      "T \n",
      "\n",
      "[20m 44s (3550 71%) train loss: 1.7058, test_loss: 1.8259]\n",
      "Wher, net re hear the cindet ins, see;\n",
      "And this the predate and to demanfer.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Martice  \n",
      "\n",
      "[21m 1s (3600 72%) train loss: 1.7235, test_loss: 1.8232]\n",
      "What so being mo; and be the dirt,\n",
      "Do coven ant so and lord to hangt shall\n",
      "sail the partiges of him ot \n",
      "\n",
      "[21m 18s (3650 73%) train loss: 1.7293, test_loss: 1.8588]\n",
      "Wheinf noke.\n",
      "\n",
      "CLAUDIO:\n",
      "Calling of are sapelire and to me;\n",
      "And as your blood and ear take exeraling,\n",
      "Mo \n",
      "\n",
      "[21m 36s (3700 74%) train loss: 1.6804, test_loss: 1.8514]\n",
      "Whis deeans made signellow.\n",
      "\n",
      "SIANENUS:\n",
      "No as here sovesty would and, juck refore\n",
      "Many are the lese-men \n",
      "\n",
      "[21m 53s (3750 75%) train loss: 1.7225, test_loss: 1.8167]\n",
      "When\n",
      "I he atsing thesees too more this besces'd deo.\n",
      "\n",
      "First:\n",
      "and hade to be here: he plood for my thou \n",
      "\n",
      "[22m 10s (3800 76%) train loss: 1.7001, test_loss: 1.8435]\n",
      "What ever the are comence true. a dead should and the ear.\n",
      "O wooling of the belowly; be sich our graun \n",
      "\n",
      "[22m 28s (3850 77%) train loss: 1.6867, test_loss: 1.7998]\n",
      "Where not iment:\n",
      "I have he betted that fantarss and then\n",
      "Than man clase and trien shall I see not scre \n",
      "\n",
      "[22m 45s (3900 78%) train loss: 1.7131, test_loss: 1.8095]\n",
      "Which and reagor, it heir ears of his beagay!\n",
      "\n",
      "First,\n",
      "Sindles mulks in thy matice, and deting he thoub \n",
      "\n",
      "[23m 3s (3950 79%) train loss: 1.6547, test_loss: 1.8094]\n",
      "What should your trought,\n",
      "Thee sange't meath in the ding here tare theak\n",
      "How his leads voadh. That whe \n",
      "\n",
      "[23m 21s (4000 80%) train loss: 1.6861, test_loss: 1.8254]\n",
      "What the patise eave you,\n",
      "That conertared we a blay a see.\n",
      "\n",
      "MENENIUS:\n",
      "Sall of my forties!\n",
      "\n",
      "CORIOLANUS: \n",
      "\n",
      "[23m 39s (4050 81%) train loss: 1.6523, test_loss: 1.8149]\n",
      "When this gessation, thee more in that\n",
      "I noblanged thou fercet my seeg the had;\n",
      "And the soverither of  \n",
      "\n",
      "[23m 56s (4100 82%) train loss: 1.6937, test_loss: 1.8056]\n",
      "What must the noth opten cambonacion:\n",
      "Let lost solly that thee heaves to be saces:\n",
      "Well with thou Righ \n",
      "\n",
      "[24m 14s (4150 83%) train loss: 1.6847, test_loss: 1.7992]\n",
      "What she strou plase!\n",
      "\n",
      "LUDIO:\n",
      "Meching, the wraths to me thee her againty in the.\n",
      "\n",
      "MERCUSIO:\n",
      "That prove \n",
      "\n",
      "[24m 31s (4200 84%) train loss: 1.6534, test_loss: 1.7859]\n",
      "What I hable.\n",
      "\n",
      "MENENIUS:\n",
      "To seal, no I the hought thesemply,\n",
      "To mull the seep herving and thy offerous \n",
      "\n",
      "[24m 48s (4250 85%) train loss: 1.6509, test_loss: 1.7559]\n",
      "What to my lance, and themere,\n",
      "Cazence and the knight with my warding if is peritiasce,\n",
      "So coutnet mit \n",
      "\n",
      "[25m 6s (4300 86%) train loss: 1.6512, test_loss: 1.7539]\n",
      "Whis in'st me sup in to bies,\n",
      "And so that for he that you to lived him:\n",
      "And sir this linders a prowbra \n",
      "\n",
      "[25m 23s (4350 87%) train loss: 1.6362, test_loss: 1.7511]\n",
      "Wher, when you ar shall he with wather,\n",
      "Here ought in thath be tump heart thou head,\n",
      "Where prosaman no \n",
      "\n",
      "[25m 42s (4400 88%) train loss: 1.6565, test_loss: 1.7318]\n",
      "What for round blood houd cose of me's poon!\n",
      "O, the you have strate as a maring them\n",
      "And to the anwind \n",
      "\n",
      "[25m 59s (4450 89%) train loss: 1.6430, test_loss: 1.7262]\n",
      "Whis all your fawer. He call and falling them;\n",
      "Coforin. for I courtes for your him?\n",
      "Whyought you arrac \n",
      "\n",
      "[26m 17s (4500 90%) train loss: 1.6428, test_loss: 1.7532]\n",
      "Whow a comence's daming swraned in the hagh\n",
      "Assall be may.\n",
      "\n",
      "LETERTIO:\n",
      "O thisbe, which that our lay\n",
      "And \n",
      "\n",
      "[26m 34s (4550 91%) train loss: 1.6273, test_loss: 1.7594]\n",
      "Where; aghise, and My vony IWalf,\n",
      "The blood your friend in chadias with the joy\n",
      "To yeur reath deand of \n",
      "\n",
      "[26m 52s (4600 92%) train loss: 1.6349, test_loss: 1.7876]\n",
      "Where, I dist we strake of your hace,\n",
      "And I have spaith.\n",
      "\n",
      "MARINAN:\n",
      "It you rod our arrish office of you \n",
      "\n",
      "[27m 9s (4650 93%) train loss: 1.5963, test_loss: 1.7472]\n",
      "Where sting ut my consiced so,\n",
      "Thy love the doy where us his forrow the tid;\n",
      "You heard your of my long \n",
      "\n",
      "[27m 26s (4700 94%) train loss: 1.5897, test_loss: 1.7544]\n",
      "What be play but thou couse and come, have\n",
      "For maken good one, or such; he reptase\n",
      "For he secking'd, b \n",
      "\n",
      "[27m 44s (4750 95%) train loss: 1.6162, test_loss: 1.7597]\n",
      "Where' since dowents on the eloust somalsul:\n",
      "I know him all of his mithter rush; mean,\n",
      "And my ere poor \n",
      "\n",
      "[28m 2s (4800 96%) train loss: 1.6419, test_loss: 1.7844]\n",
      "Whing to more thy suctleme:\n",
      "The come I be burder fith fith both a all.\n",
      "\n",
      "HARTING:\n",
      "So you, these knays L \n",
      "\n",
      "[28m 20s (4850 97%) train loss: 1.5986, test_loss: 1.7343]\n",
      "What is no more and from the charned\n",
      "With my heart plite of trayul west diend ay I rest\n",
      "dit.\n",
      "\n",
      "KING HEN \n",
      "\n",
      "[28m 37s (4900 98%) train loss: 1.5941, test_loss: 1.7586]\n",
      "Whis meary to my dies,\n",
      "And that he intingly king, gentry ere,\n",
      "In hope dead-nand to reer beed the prost \n",
      "\n",
      "[28m 54s (4950 99%) train loss: 1.6046, test_loss: 1.7477]\n",
      "Whis wnot somer's deattand not her but the ichold\n",
      "I can the corsure if the earted was this\n",
      "Tisleps of  \n",
      "\n",
      "[29m 12s (5000 100%) train loss: 1.5827, test_loss: 1.6855]\n",
      "Where sout that with by do enkering will;\n",
      "And power him forter her you hive less of this,\n",
      "To king\n",
      "Than \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers, bidirectional=True, dropout=0.5).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save network\n",
    "#torch.save(classifier.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa8f0332bb0>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAmmElEQVR4nO3deXhd1X3u8e9P52geLUuWhSVZnrENHrAxZkgwMxgKFEhCZloS0tw0SZvkaZq2yW3S4Una3jTQ3JuUhjZAwhwIQ5jBzGAjj9jYBo/yJGuWrFlHWvePdWTLQrJlW9LWOXo/z7MfnWGfs3+bbV4trb322uacQ0REYl9C0AWIiMjQUKCLiMQJBbqISJxQoIuIxAkFuohInAgHteG8vDxXWloa1OZFRGLS6tWrq51z+f29F1igl5aWUlZWFtTmRURikpntHug9dbmIiMQJBbqISJxQoIuIxAkFuohInFCgi4jECQW6iEicUKCLiMSJmAv0rRWH+LfntlLb3BF0KSIio0rMBXrTQ7/jizcupXbdpqBLEREZVWIu0JOTwuQ319NRUxt0KSIio0rMBXpiTjYAHXX1wRYiIjLKxFygJ+fmABCpbwy2EBGRUSbmAj0l17fQuxoaAq5ERGR0icFAHwdAd4Na6CIivcVcoKfl5wLgGhXoIiK9xVygJ2WkEbEEaDwUdCkiIqNKzAU6ZjSnpJHQpEAXEekt9gIdaE1OI6RAFxE5SmwGeko64eamoMsQERlVYjLQ21PTSWxRoIuI9BaTgd6RlkFSS3PQZYiIjCoxGeid6RmktCrQRUR6G3Sgm1nIzNaa2VP9vJdsZg+a2TYzW2lmpUNaZR+R9AxS2xToIiK9nUgL/ZvA5gHeuxWoc85NB/4d+MmpFnYs3RkZpLa1DucmRERizqAC3cyKgKuBXw2wynXA3dHHjwCXmJmdenn9cxmZZHS04Lq6hmsTIiIxZ7At9J8BfwV0D/D+JGAPgHMuAjQA4/uuZGa3mVmZmZVVVVWdeLVRLisLgPYGjUUXEelx3EA3s2uASufc6lPdmHPuTufcYufc4vz8/JP+HsvMBKC1Wje5EBHpMZgW+vnAtWa2C3gAuNjMftNnnX1AMYCZhYFsoGYI6zxKQvQmF221mkJXRKTHcQPdOfc951yRc64UuBl42Tn3uT6rPQF8Mfr4pug6bkgr7SWc7btc2mrrh2sTIiIxJ3yyHzSzHwFlzrkngLuAe81sG1CLD/5hkzguB4CO2rrh3IyISEw5oUB3zr0CvBJ9/INer7cBnxjKwo6lJ9A7dRs6EZHDYvJK0eRc3+USqVcfuohIj5gM9MO3odNdi0REDovJQE8b7wPd6UbRIiKHxWSgp2el05EQhkO6sEhEpEdMBnpSYojm5FTdV1REpJeYDHSAFt2GTkTkKLEb6CnphHQbOhGRw2I20NtT00lUoIuIHBbTga7b0ImIHBGzgd6Zlk5yq1roIiI9YjbQI+mZpLS1BF2GiMioEbOB3pWh+4qKiPQWs4HenZFJWkcb6DZ0IiJADAd6z23onK4WFREBYjjQe25D166bXIiIALEc6IfvWqQJukREIIYDPRQN9PYa3bVIRARiONATozeKbq+rD7YQEZFRImYDPTzOB3qHAl1EBIjhQE/OzQEgUqc+dBERiOlA93ct6tJt6EREgBgO9NRoC727QYEuIgIxHOjpmWm0hxJBLXQRESCWAz05RFNSqu4rKiISFbOBnhwO0ZychjVpCl0REYjhQAdoTUkjrPuKiogAMR/o6YSbFegiIhDjgd6Wlk6ibkMnIgIMItDNLMXMVpnZejPbZGY/7GedW8ysyszWRZcvDU+5R+tMTSe5RX3oIiIA4UGs0w5c7JxrMrNE4A0ze8Y5906f9R50zv350Jc4sM70TJJbdRs6EREYRKA75xzQ0wxOjC5uOIsarK503YZORKTHoPrQzSxkZuuASuAF59zKfla70cw2mNkjZlY8lEUOpCsjg5TOdohERmJzIiKj2qAC3TnX5ZxbABQBS8zsjD6rPAmUOufmAS8Ad/f3PWZ2m5mVmVlZVVXVKZQdrSt61yJdXCQicoKjXJxz9cAK4Mo+r9c459qjT38FLBrg83c65xY75xbn5+efRLl99NxXtEEzLoqIDGaUS76Z5UQfpwKXAVv6rFPY6+m1wOYhrHFACdFA79AUuiIigxrlUgjcbWYh/C+Ah5xzT5nZj4Ay59wTwDfM7FogAtQCtwxXwb31BHr7gYMkLxyJLYqIjF6DGeWyAfhIXDrnftDr8feA7w1tacfXvGARh5JSCf/XnbD88pHevIjIqBLTV4omTcjj14uuJfXxR2HjxqDLEREJVEwHekZymF+dfT1daenwox8FXY6ISKBiOtAXluSQMD6XJ5fdBA8/DO+9F3RJIiKBielAz0xJ5OsXz+Dvp11BJCNTrXQRGdNiOtABPrd0MtmTCnjovBvgkUdg7dqgSxIRCUTMB3pSOIHvXDGLH8++irbcPPjKV6CrK+iyRERGXMwHOsA1ZxZSOn0S/3T5V+Ddd+H224MuSURkxMVFoCckGN+7ajb3lixl25Jl8Hd/Bzt2BF2WiMiIiotABzh32ng+eXYxX1h8C12hkO96caNill8RkRERN4EO8P1r5pBQUswdl94KL74Ijz0WdEkiIiMmrgI9MyWRn35yAf8x6xIax+XDb38bdEkiIiMmrgIdYMmUXL788en8vnQJXU8/A826o5GIjA1xF+gA37p8Jm8tuJBQWys8+2zQ5YiIjIi4DPTkcIipN15FbWoWrQ88FHQ5IiIjIi4DHeBTS6fw/IylhJ7+A7S3H/8DIiIxLm4DffL4dMqXXUlSSzNdzz0fdDkiIsMubgMdYM7n/5jG5HQqf31f0KWIiAy7uA70yxYU8/rpS8l6/mno7Ay6HBGRYRXXgZ4cDtFyzXWkNzdS97S6XUQkvsV1oAMsuu1mWsPJ7LtHo11EJL7FfaBPLcnnvRkLyH3zlaBLEREZVnEf6ACHPnYRpx0sp3HLh0GXIiIybMZEoE/4xLUA7PrtowFXIiIyfMZEoM9etoQDWfnwvE6Mikj8GhOBHg6H2LnoAqZsWEl3h4Yvikh8GhOBDhC+6goy25rZ/tRLQZciIjIsxkygz/z09XRZAjWPPhl0KSIiw2LMBHpOUQEfls5h3Bsrgi5FRGRYjJlAB2j82EXM2L2Fql37gi5FRGTIHTfQzSzFzFaZ2Xoz22RmP+xnnWQze9DMtpnZSjMrHZZqT1H+jdeSgOPD+x4PuhQRkSE3mBZ6O3Cxc24+sAC40syW9lnnVqDOOTcd+HfgJ0Na5RApvepCGlMz4Zmngy5FRGTIHTfQndcUfZoYXVyf1a4D7o4+fgS4xMxsyKocIpaYyI7FH+P0Na/T1tYRdDkiIkNqUH3oZhYys3VAJfCCc25ln1UmAXsAnHMRoAEY38/33GZmZWZWVlVVdUqFn6zE668lt6WR9x97IZDti4gMl0EFunOuyzm3ACgClpjZGSezMefcnc65xc65xfn5+SfzFads2udvojMhxKFHHgtk+yIiw+WERrk45+qBFcCVfd7aBxQDmFkYyAZqhqC+IZeSP57tsxZS9MZLONe350hEJHYNZpRLvpnlRB+nApcBW/qs9gTwxejjm4CX3ShOy7YrlzOtchcfvLMh6FJERIbMYFrohcAKM9sAvIvvQ3/KzH5kZtdG17kLGG9m24BvAX89POUOjZIvfgqAA795JOBKRESGjgXVkF68eLErKysLZNsAeydOpipnAgu3vBtYDSIiJ8rMVjvnFvf33pi6UrS3qgsvY+6H66jcWxl0KSIiQ2LMBnrep28iqTvC1nt+F3QpIiJDYswGetHVl9CQlkXCk08EXYqIyJAYs4FuiYnsPu8Szlz7OnV1Tcf/gIjIKDdmAx0g+7OfJKu9mQ336iIjEYl9YzrQSz51HS1JqXT/Tv3oIhL7xnSgW2oqu8+9iHllr1Db2Bp0OSIip2RMBzpAxqc/yfiWBtbcp1vTiUhsG/OBXvSZG2gPJ9H1iLpdRCS2jflAt8xMys/+GGeuepkadbuISAwb84EOkPbpT3LaoWreffi5oEsRETlpCnTgtM99gkhCiI6HNFmXiMQuBTpg48axd9H5nPXOC+yvbQ66HBGRk6JAj0r/0y9S1FjJqt9qtIuIxCYFelT+5z5JW1IKifffH3QpIiInRYHeIyOD/cuu4Py1L7OlvDroakRETpgCvZe8r/wpOW1NbPrVg0GXIiJywhTovWRdu5zGzHHkPPYQ3d2j9paoIiL9UqD3Fg5TffX1nL/lHVa/tyvoakRETogCvY/TvvYlUiId7Lrz3qBLERE5IQr0PlLOP5fqiSWUPvUwdc0dQZcjIjJoCvS+zHBf+Qpnl2/khV/r9nQiEjsU6P3I//bXaU7LJO//3U5bZ1fQ5YiIDIoCvT+ZmdTe8iWWvf8mLz72atDViIgMigJ9AEU/+C6diYkk/J9/0xBGEYkJCvQBWEEB+274NJeWvcCbr6wLuhwRkeNSoB9DyT9+nwS6qf2nn6iVLiKjngL9GMLTp7H3iuu48tVHefb3bwRdjojIMSnQj2PynXfQlZhIzl/9JdWH2oIuR0RkQMcNdDMrNrMVZva+mW0ys2/2s84yM2sws3XR5QfDU+7Is6Iimv/uf3Pe9tX84ft3BF2OiMiAzLlj9w2bWSFQ6JxbY2aZwGrgeufc+73WWQZ8xzl3zWA3vHjxYldWVnZSRY+4SISDs+dDRQU7X3uXpQunBl2RiIxRZrbaObe4v/eO20J3zh1wzq2JPj4EbAYmDW2Jo1w4TM49d5HfXMe+r32L+hZNCSAio88J9aGbWSmwEFjZz9vnmtl6M3vGzOYO8PnbzKzMzMqqqqpOvNoAJZ+7lOpbvsyNbz/Og1/9IZ1d3UGXJCJylEEHupllAL8D/sI519jn7TXAZOfcfOA/gN/39x3OuTudc4udc4vz8/NPsuTgTPjPn1Nx7oXceu+PeeAHvwi6HBGRowwq0M0sER/mv3XOPdr3fedco3OuKfr4aSDRzPKGtNLRIDGRic8/RdX0Odz0L9/mmTs/8p9CRCQwgxnlYsBdwGbn3E8HWGdidD3MbEn0e2uGstBRIyODCa+9SOP4CZz3jS/w6n8+FHRFIiLA4Fro5wOfBy7uNSxxuZn9mZn9WXSdm4CNZrYeuAO42R1v+EwMC00sIPv1VziUO4Hz/tdneOdv/yXokkREjj9scbjE1LDFAbRV17Jt2XLO2LSSDTd/mXm/+QWEQkGXJSJx7JSGLcrAUvJymVn2Gq9e8gnmPfBfbF90AZ0HK4MuS0TGKAX6KUpKSeL85x7gqa//iKKNq2mYPY/GV98MuiwRGYMU6EMgHErgmju+z2v/8xht3Y7US5ax/7s/gEgk6NJEZAxRoA+hyz5/NdWvvMVrs8/jtH/5ByrOXERk46agyxKRMUKBPsQWLJjG4lUv8j/f+DFJu3fRvfAsqv/6+9Ch6QJEZHgp0IdBdmoif3L7d1n15KusmLWUvJ/8I9XTZ9Py0itBlyYicUyBPoyuvGQBi995nv/+3s9pa2gi7dKLOHjhZbi33w66NBGJQwr0YTY+I5k//eevUfXOau656laSVq3EzjuPto9dCG9qNIyIDB0F+ghZOLuIzzx5J4///k1+fNltNK57Dy64gK4bboRt24IuT0TigAJ9BIVDCdxyxZl84aGf8c8/e4KfXvBZ2v/wNN2z5+Bu+RNYty7oEkUkhinQA3BaTio/u/UCLrjnDv7s+/dz77zLab//AVi4EPfxj8NzzwVdoojEIAV6gJZMyeXXf3MdWb/6JTf8zUP8w0W3UrXpQ7jySrj8cli7NugSRSSGaHKuUaKzq5uHyvbwy+fe59LXHuPb7zxIessh7JJL4I/+yC9TpgRdpogE7FiTcynQR5m2zi7ueXsX9z6znptee5hP7nyHwgO7/Jvnnw9///dwySXgp58XkTFGgR6DGts6uev1nfz6rV3k7N/Nl6o38Ik3HyXl4H4f7N/6FlxxBaSnB12qiIwgBXoMa26P8MC7e7jr9R1U1xzi1g9W8NW3HyKrugJSUuDSS+Haa+Gaa6CwMOhyRWSYKdDjQKSrmxVbq7h/VTlvvL+fs/ds4kt173H+pjdJ2rvHr7R4Mdx4I9x6K8TgTbhF5PgU6HFmf30rd7+1i/tWlnOorZM/TqzjS/UbmV32KgkrV0JyMnz2s/C1r8HChepvF4kjCvQ41dQe4cF39/Cbd3azs7qZnLREvprfxmdWPUHmQ/dDSwtMnQrXXw833ADnngsJGqkqEssU6HGuu9vxzo4afrNyN89vOkiXc1xdlMKf161n1psvYi+/5KfvnTQJbrrJL0uXQjgcdOkicoIU6GNIRUMb960q576V5VQ3tTMpJ5UvzB3HzZXryX7qcXjmGWhvh+xsP/zxiivg6qt92IvIqKdAH4M6It08t6mC+1eV89b2GkIJxoUz8/nUzGwu2r2WpJde8FMM7N3rP7BokR8t86lPwaxZwRYvIgNSoI9xO6qaeHj1Xh5bs4+KxjayUsIsP7OQa+cXck7rQUJ/eAqeeALeeQec8+F+882+z33uXMjJCXoXRCRKgS4AdHU73tpezaNr9vH8pgqaO7ooyErmqjMKuXpeIYvCrSQ88jDcdx+8++6RDxYV+YuZLr3UL6Wlge2DyFinQJePaO3o4sXNB3li/X5e/aCKjkg3BVnJXL9wEp9YVMT09nrYsAE2bfLT+r7yChw44D88YwZcdZVfLrwQUlMD3BORsUWBLsd0qK2Tl7dU8uT6/azYWkVXt2N+cQ5/NK+Qq84sZFJOqu+K2bIFXngBnn0WVqyAtjZ/tepFF8Hy5T7cZ8/W6BmRYaRAl0GrOtTO4+v28eiafbx/oBGA+UXZXDhrAudNG8/CkhySwyFobYXXXvOjZp5+Gj780H9BaiosWABLlvg++HPPheJiXdwkMkQU6HJSdlU38+ymCp7dWMGGvfV0O0hJTODyORP57DklLJmSi/UE9fbt/qRqWZlfVq/2oQ9+SOR55/l++PPOg/nzISkpuB0TiWEKdDlljW2drNpRy6sfVPH7dfs41BZhxoQMLp49gTmFWcwpzGJqfgahhGjAd3b6Pvi33vLLm2/CnuicM8nJfkqCc85RK17kBJ1SoJtZMXAPUAA44E7n3O191jHgdmA50ALc4pxbc6zvVaDHrtaOLp5cv58H3i3nvX0NdHb5f0N5GclcMbeAq84oZOnUXMKhPtMM7NnjW/GrVsHKlb4l39OKnzgR5s2DM8/0wyaXL/cXP4nIUU410AuBQufcGjPLBFYD1zvn3u+1znLg6/hAPwe43Tl3zrG+V4EeHzoi3WyvamLT/kZWbKnk5S2VtHZ2kZuexBVzJ3L1mQOEOxzdii8rg/feg/ff91eyJif7UL/hBn+itbQUcnPVipcxb0i7XMzsceDnzrkXer32n8Arzrn7o8+3AsuccwcG+h4Fenxq7eji1Q8qefq9Cl7cfJCWji5SEhOYXZjF3NOyOGfKeK6YO5Gk8ACThEUifgz8gw/6paLiyHtZWXDWWf6E69ln+xb9tGkQCo3MzomMAkMW6GZWCrwGnOGca+z1+lPAj51zb0SfvwR81zlX1ufztwG3AZSUlCzavXv3Ce6KxJKecF+1s46N+xvYvL+RQ+0R8jKSuPnsEm5cVETp+LQjJ1b76uryrfadO2HXLj+SpqwM1q/3k42BHzY5ezZMnuwvgCoq8iddly7VFa4Sl4Yk0M0sA3gV+Cfn3KN93htUoPemFvrY093teH1bNfe+vYuXtlTiHGSnJnLGpCzmF+Vw/vQ8Fk0eR0ricVrc7e0+6Ddu9MumTb5/ft8+qK/365j5aQtmz4aSEr/MmeP758eNG/Z9FRkupxzoZpYIPAU855z7aT/vq8tFTsjeuhZe2VrFpv0NbNzXyOYDjUS6HSmJCZwzZTyXzy3gsjkFTMhMObEvbmz0rfi33oK33/bDKcvLj5x8BZg+3ffJZ2T4e7JOm+anNDjnHA2nlFHvVE+KGnA3UOuc+4sB1rka+HOOnBS9wzm35Fjfq0CX3praI6zcUcPrH1bzytZKdtW0YAbzi3KYe1oWsyZmMqsgk4Ul4wbufx+Ic1BV5U/AlpX5PvoDB6CpyS+7d0N3tw/4s8/2gT9tmp918swzYcoU3RhERo1TDfQLgNeB94Du6Mt/A5QAOOd+GQ39nwNX4oct/smxultAgS4Dc87xwcEmnt1YwRvbqthacYjGtggA6UkhLpiRx0WzJrB06ngmH6sPfrDq6vxcNS+84Oet2bbN/wLokZ7uu26mTPEt+57++uJi/3j8+FPbvsgJ0IVFEtOccxxsbGfD3npe+aCKFVsqOdDQBvix72eXjmNxaS5LSnOZXZjZ/xDJE9XY6Oeuee8937LfvNm35HftOnJCtsfkyb5lv2iR76svLPRXx5aWqgtHhpwCXeKKc45tlU28u6uOsl21rNpVy94630eekRzmvGnjuXR2ActOzz/xPvjj6e6Ggwf9Cdg9e3xrvqcbZ+fOo9cNhXyr/vTT4eMfh8sv90MtNZZeToECXeLegYZWVu2sZeXO2qNa8CW5acyYkMH0ggym52cwNT+Dafnp5KQNQ8u5sRH27/f983v2+GGWW7f6kTibN/t1JkzwLfpx4/ySlOSDPxTy3Thz58IZZ/gpijVrpfRDgS5jinOOzQcOsWJrJZsPNLKtsokdVc10dHUfXmdSTipnl47j7Cm5zCzIJCc1key0RHLTkoamy6av/ft9H/2KFf5iqbo6P8Syo8OPt49E/Os9/z+mpPjW/MKFcNppR1r1WVn+hO3UqX5JGeK/QGTUU6DLmBfp6mZPXSs7qprYVtnE+r31rNpZR3VT+1HrZaaEuWjWBC6dU8CFM/PJTk0cuSJbWny//caN/uKpNWtg7VpoaOh//XDYt+jPOssvPXPhaJx9XFOgi/TDOcfumhbKa1toaO2kvqWDDXsbeHlLJTXNHZjBrIJMFpeOY15RDkU5qRTmpFKYnXL8i5+Grkjfb9+jttaPrd++3c97s2aNn6q496icnBzf4u/o8C370lLfqp8yBfLy/KicvDw/PHPmTE2CFmMU6CInoKvbsba8jje31VC2u5a15fU0tUcOv58UTuDCmfksP3Mil8wuICtlBFvx/XHOd+n0jMgpL/d980lJPth37oQdO/wInZ4raXubMMEHfHa2/2UwcaIfpdMzlUJxsV9ycnRCdxRQoIucgq5ux57aFvY3tLK/vo2N+xp4dmMFFY1tmEFhVgqTx6czJT+deZOyWVCSw4wJmUfmhh9NIhEf6gcPHjlpu22b79NvaPA/Dxzw/fm9/zIAf+FVSYkP94yMI/39kyYdmdd+8mSF/jBToIsMse5ux9o99bzxYTW7a5rZWdPM9sqmwxdApSWFmD4hg2n5GUzNS2dyXjoluWmU5KaRmx4DY9N7TtLu3etH7PQs5eUfnUph507f/w++O2fuXL+UlvqLstLSjiypqf5nz7QL2dn+M/olMGgKdJER4JxjZ3Uz6/bUs2FvA9urmthe2cT+6BDKHhOzUphfnM28ohym5qVTkJ1CYXYKEzJTRmer/ngiEd/d8/bb/krbTZv8MtDJ3L5SUnyrf9IkP+d9To4fzRMK+SkXwmF/sdbkyX45/XQ/X/4YpUAXCVBLR4S9da2U17Swq6aZjfsaWL+3gZ3VzUetlxRKoCg3lcm5acyamMX8omzmF+dQmJ1y6tMbjDTnfKu9ufnIz9ZWvzQ3H1lqa4/8FdAzW2Z9vR/T393tl85O/0ujR2KiH9GzeDHk5/tWf2qqb/H3LMnJfr1w2P8FUFzs/xqItf+O/ThWoOvKBZFhlpYUZmZBJjMLMo96vaG1k311rVQ0+r75PXUt0dBv4Y1tOw7f2i8nLZFp0QuiZhZkMrswi9mFWaO768bsSLieqp7J1crL/cndNWv81bkPPeTDf7CN0owM/1fApEl+bH96OrS1+V8yycm+i6j3fD3Fxf6XQgxRC11kFGqPdLHlwCHW761na8UhtlU2sb2qieqmI/PI5KYnkZeRRF5GMhOzUijNS6c0L53p+RnMLMgYngukRhvn/PDM3i3/pib/WmenX6qrj/4rYP9+/7OlxbfsU1L84337jj4RnJDgu3omTvQ/8/N9/39KypETxFOm+Au8iopG7M5Z6nIRiRPVTe1sPuDnj99d00J1UzvVTR3sr289PN0BQGpiiHlF2SwozqE0L53J49MoHZ8em903I6Wjwwf/zp1+Irbdu/0vgYoKv1RVHek2am09+i+DcPjICKDmZqip8SOGcnOPDP/sCf9p0/zsnRMmnFSZCnSRMaC1o4vdtc1srTjE2vJ61pbX8f6BxsNdN+CnH55ekMn0/AyKc1OZlJPKpHGpFOWkMTE75cTnmh+rOjt92O/ceWTZscO/lpnpx/Xn5Bx9jqC83E/zAPCd78C//utJbVqBLjJGdXU79te3Ul7bwo5qP7Tyg4O+C6fy0NHTHphBQWYKxbmplOT6Vn1+ZjI5qYnkpCUxLT+dCVmaO+ak9fwS2L7d9+HPnXtSX6NAF5GPaI90UdHQxr66VvbWt/qfda3sqW1hd20zBxvbP/KZvIxkzpiUxcyCTKbkpTMlL52ctMTDvQ9mEE5IIJxg5GYkBX8VbRzSKBcR+YjkcIjJ49OZPL7/kShtnV3UtXRQ19xJbXMHH1YeYuO+Rjbtb+Ct7TV0RLr7/VxvU/PSmV+cw+zCTE7LSaUw23fzTMhMJiEWx9yPcgp0EelXSmKIwmwfwgAXzMg7/F5PV86ummaaolfHmkG3g0i3I9LVzYGGNtbtqefNbdU8tnbfUd+dHE6gJDeN0rx0puVnMH1CBqXj00hPDpOaGCIrNXF0D8scpRToInLCQglGcW4axblpg1q/obWTAw2tHKhvY299K+U1zeyqaWFXdTOvbK086sRtj6JxqSyZksvC4hy6uh0NrRGaOyLkZSQxKSeNonGpzCzIJDVphGa+jAEKdBEZdtmpiWSnJnL6xKyPvNfZ1c2eWj+NcWtHF62dXdQ0dbB6dx2vbq3i0TVHWvdJoYSjblQSSjBmFWQyvzibiVmppCeHSE8Ok5WSeHibuRlJ5Gckj4kRPAp0EQlUYiiBqdHbA/b2Zfz8OBWNbSSHQ2SmhEkMJRy+wnZPXQsb9zWwbk89z2ysoL6l85jb6bkQa1yaX/IzkynMSWFSTioTs1LIy0wmPzOZzORwzI7V1ygXEYkLka5umju6aGqPcKitk4aWThpaO6lp7qCysZ3KQ23UNHVQ29JBbXMHlY1th2fH7C0zJcy8omzmF+UwoyADw+jqdiQkQGayv1VhZkqYlHCIlMQQacmhER3No1EuIhL3wqEEslMTorcNTB3UZ5rbIxxoaKWioT161W07O6qb2bC3njtf20Gke3AN3qn56ZwzZTxnl45jXFoS4ZCRGEpgXJr/qyAnLWlEZtJUoIvImJWeHGb6hEymT8j8yHttnV3sq28lwYyE6AieQ22dNLZGaGzrpK2zi/ZIN7XNvr//qfX7uX9Veb/bSTC/rbSkEGlJYT57Tglf+tjUId8fBbqISD9SEkNM69Ovfyxd3Y7tVU20dHTR2dVNR6SbupYOqg+1U9PcQVN7hJb2Llo6u8jPHJ753BXoIiJDIJRgH5kieaTF/zgeEZExQoEuIhInjhvoZvbfZlZpZhsHeH+ZmTWY2bro8oOhL1NERI5nMH3ovwZ+DtxzjHVed85dMyQViYjISTluC9059xpQOwK1iIjIKRiqPvRzzWy9mT1jZic3a7uIiJySoRi2uAaY7JxrMrPlwO+BGf2taGa3AbcBlJSUDMGmRUSkxym30J1zjc65pujjp4FEM8sbYN07nXOLnXOL8/PzT3XTIiLSyym30M1sInDQOefMbAn+l0TN8T63evXqajPbfZKbzQOqT/KzsWws7vdY3GcYm/s9FvcZTny/Jw/0xnED3czuB5YBeWa2F/jfQCKAc+6XwE3AV80sArQCN7tBTOHonDvpJrqZlQ0021g8G4v7PRb3Gcbmfo/FfYah3e/jBrpz7tPHef/n+GGNIiISIF0pKiISJ2I10O8MuoCAjMX9Hov7DGNzv8fiPsMQ7ndgdywSEZGhFastdBER6UOBLiISJ2Iu0M3sSjPbambbzOyvg65nOJhZsZmtMLP3zWyTmX0z+nqumb1gZh9Gf44LutbhYGYhM1trZk9Fn08xs5XRY/6gmSUFXeNQMrMcM3vEzLaY2WYzO3csHGsz+8vov++NZna/maXE47Hub8bagY6veXdE93+DmZ11ItuKqUA3sxDwf4GrgDnAp81sTrBVDYsI8G3n3BxgKfC16H7+NfCSc24G8FL0eTz6JrC51/OfAP/unJsO1AG3BlLV8LkdeNY5dzowH7/vcX2szWwS8A1gsXPuDCAE3Ex8HutfA1f2eW2g43sVfuqUGfhpUn5xIhuKqUAHlgDbnHM7nHMdwAPAdQHXNOSccwecc2uijw/h/wefhN/Xu6Or3Q1cH0iBw8jMioCrgV9FnxtwMfBIdJW42m8zywY+DtwF4JzrcM7VMwaONf46mFQzCwNpwAHi8FgPMGPtQMf3OuAe570D5JhZ4WC3FWuBPgnY0+v53uhrccvMSoGFwEqgwDl3IPpWBVAQVF3D6GfAXwHd0efjgXrnXCT6PN6O+RSgCvifaDfTr8wsnTg/1s65fcC/AeX4IG8AVhPfx7q3gY7vKWVcrAX6mGJmGcDvgL9wzjX2fi86vUJcjTk1s2uASufc6qBrGUFh4CzgF865hUAzfbpX4vRYj8O3RqcApwHpfLRbYkwYyuMba4G+Dyju9bwo+lrcMbNEfJj/1jn3aPTlgz1/fkV/VgZV3zA5H7jWzHbhu9Muxvcv50T/LIf4O+Z7gb3OuZXR54/gAz7ej/WlwE7nXJVzrhN4FH/84/lY9zbQ8T2ljIu1QH8XmBE9E56EP4nyRMA1Dblov/FdwGbn3E97vfUE8MXo4y8Cj490bcPJOfc951yRc64Uf2xfds59FliBnwQO4my/nXMVwB4zmxV96RLgfeL8WOO7WpaaWVr033vPfsftse5joOP7BPCF6GiXpUBDr66Z43POxdQCLAc+ALYDfxt0PcO0jxfg/wTbAKyLLsvx/ckvAR8CLwK5Qdc6jP8NlgFPRR9PBVYB24CHgeSg6xvifV0AlEWP9++BcWPhWAM/BLYAG4F7geR4PNbA/fjzBJ34v8huHej4AoYfybcdeA8/CmjQ29Kl/yIicSLWulxERGQACnQRkTihQBcRiRMKdBGROKFAFxGJEwp0EZE4oUAXEYkT/x9G13NG3GhdKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thy many\n",
      "To my land best there of whirse I call the foull\n",
      "be him, not you will mine thou wellor to king\n",
      "Made the songamith, that we heaven wad,\n",
      "Even the morty all even that endrish me\n",
      "Good-un fithter the will bled of in the mears\n",
      "With princely boiness with that hind consent\n",
      "And light be fanger but a greath are of man\n",
      "the with me that hath adon the murming and hanged\n",
      "And a man'st to fremone, sumest to the gliet.\n",
      "Come, I am it be but againes and worsh,\n",
      "Or speap not my marry, and both tho?\n",
      "\n",
      "ETHARS:\n",
      "I was neveres in the spapely if a alls\n",
      "Brit not all this wonder worsting futless in hear to\n",
      "being in hure a not parton of this off\n",
      "Of warwick comaning yours, and not sucizen\n",
      "Uncest marrones to the doint to have grace:\n",
      "Go do be with the bust.\n",
      "\n",
      "EDOLD:\n",
      "Me like the save tongue--\n",
      "Harg with to be a begites of a meath;\n",
      "In hear I wave the wermand thou dotelan to the deak.\n",
      "\n",
      "EULET:\n",
      "Ave I cooh reys.\n",
      "\n",
      "Second Murderren:\n",
      "Ablast that with my lets in thing, being\n",
      "Anoted in he saave the canst that say\n",
      "Man'st too \n"
     ]
    }
   ],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
